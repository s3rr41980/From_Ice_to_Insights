{"cells":[{"cell_type":"markdown","source":["##### This Notebook loads the latest (by time stamp) raw data drop from raw_partition_zone to Delta format in Lakehouse_Bronze\n","##### Create if missing; else OVERWRITE or idempotent APPEND without duplicates"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae85b2cf-0a38-43d9-96fe-478bffb90c20"},{"cell_type":"code","source":["import os\n","from pyspark.sql import functions as F\n","from delta.tables import DeltaTable\n","\n","# SCD Type 1 (overwrite)\n","# These are dimensional tables where (1) values within (e.g., position) may change over time, (2) history not required, and (3) dataset remains relatively small over time\n","OVERWRITE_TABLES = {\n","    \"player_info\",\n","    \"team_info\",\n","}\n","\n","# Idempotent Append (upsert-by-key, fall back to hash de-dup if no keys)\n","# These are fact tables where (1) values mid-table typically does not change, (2) data gets added incrementally and grows large quickly over time\n","UPSERT_TABLES_KEYS = {\n","    \"game\": [\"game_id\"],\n","    \"game_plays\": [\"game_id\", \"play_id\"],\n","    \"game_plays_players\": [\"game_id\", \"play_id\", \"player_id\", \"playerType\"],\n","    \"game_goals\": [\"play_id\"],\n","    \"game_penalties\": [\"play_id\"],\n","    \"game_goalie_stats\": [\"game_id\", \"player_id\"],\n","    \"game_skater_stats\": [\"game_id\", \"player_id\"],\n","    \"game_teams_stats\": [\"game_id\", \"team_id\"],\n","    \"game_officials\": [\"game_id\"],\n","    \"game_scratches\": [\"game_id\", \"player_id\"],\n","    \"game_shifts\": [\"game_id\", \"player_id\"],\n","}\n","\n","# Root path to partitioned raw zone (note the trailing slash)\n","files_root = (\n","    \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/\"\n","    \"f8bc9aa5-a717-4b16-a592-33bcfe0202bb/Files/raw_partition_zone/\"\n",")\n","\n","# Helper: stable row hash across all columns (used when keys are not provided)\n","def with_row_hash(df):\n","    cols = sorted(df.columns)\n","    return df.withColumn(\n","        \"_row_hash\",\n","        F.sha2(F.concat_ws(\"||\", *[F.col(c).cast(\"string\") for c in cols]), 256)\n","    )\n","\n","# 1) List timestamped subfolders\n","entries = mssparkutils.fs.ls(files_root)\n","subfolders = [e for e in entries if e.isDir]\n","if len(subfolders) == 0:\n","    raise RuntimeError(\"No subfolders found in raw_partition_zone.\")\n","\n","# 2) Pick the latest folder (yyyy-MM-dd_HH-mm-ss sorts correctly)\n","latest_folder = sorted([f.name.strip(\"/\") for f in subfolders])[-1]\n","latest_path = files_root + latest_folder + \"/\"\n","print(f\"Latest folder: {latest_folder}\\nPath: {latest_path}\")\n","\n","# 3) Find all CSV paths\n","all_files_df = spark.read.format(\"binaryFile\").load(latest_path + \"*.csv\").select(\"path\")\n","csv_files = [r.path for r in all_files_df.collect()]\n","if len(csv_files) == 0:\n","    raise RuntimeError(f\"No CSV files found in: {latest_path}\")\n","print(f\"Found {len(csv_files)} CSVs.\")\n","\n","# 4) Process each CSV\n","for file_path in csv_files:\n","    file_name = os.path.basename(file_path)\n","    base_name = os.path.splitext(file_name)[0]\n","    table_name = base_name + \"_bronze\"\n","\n","    print(f\"\\nProcessing {file_name} → {table_name}\")\n","\n","    df = (\n","        spark.read\n","        .format(\"csv\")\n","        .option(\"header\", \"true\")\n","        .option(\"inferSchema\", \"true\")\n","        .load(file_path)\n","    )\n","\n","    table_exists = spark.catalog.tableExists(table_name)\n","\n","    if not table_exists:\n","        # First time: create table\n","        (df.write\n","            .format(\"delta\")\n","            .mode(\"overwrite\")\n","            .option(\"overwriteSchema\", \"true\")\n","            .saveAsTable(table_name))\n","        print(f\"Created {table_name} with {df.count()} rows.\")\n","        continue\n","\n","    # Table exists\n","    if base_name in OVERWRITE_TABLES:\n","        # Always snapshot overwrite for these tables\n","        (df.write\n","            .format(\"delta\")\n","            .mode(\"overwrite\")\n","            .option(\"overwriteSchema\", \"true\")\n","            .saveAsTable(table_name))\n","        print(f\"Overwrote {table_name} with {df.count()} rows.\")\n","    else:\n","        # Idempotent append for facts\n","        if base_name in UPSERT_TABLES_KEYS and len(UPSERT_TABLES_KEYS[base_name]) > 0:\n","            # Insert only new keys via MERGE\n","            key_cols = UPSERT_TABLES_KEYS[base_name]\n","            df_dedup = df.dropDuplicates(key_cols)\n","            target = DeltaTable.forName(spark, table_name)\n","            merge_condition = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in key_cols])\n","            insert_map = {c: f\"s.`{c}`\" for c in df_dedup.columns}\n","            (target.alias(\"t\")\n","                   .merge(df_dedup.alias(\"s\"), merge_condition)\n","                   .whenNotMatchedInsert(values=insert_map)\n","                   .execute())\n","\n","            metrics = DeltaTable.forName(spark, table_name).history(1).select(\"operationMetrics\").collect()[0][0]\n","            inserted = int(metrics.get(\"numTargetRowsInserted\", \"0\"))\n","            print(f\"Inserted {inserted} new rows into {table_name}.\")\n","        else:\n","            # Generic, keyless de-dup using row hash\n","            src_h = with_row_hash(df)\n","            tgt_h = with_row_hash(spark.table(table_name)).select(\"_row_hash\").distinct()\n","            new_rows = src_h.join(tgt_h, on=\"_row_hash\", how=\"left_anti\").drop(\"_row_hash\")\n","            if new_rows.limit(1).count() == 0:\n","                print(\"No new rows found (all duplicates).\")\n","            else:\n","                (new_rows.write\n","                    .format(\"delta\")\n","                    .mode(\"append\")\n","                    .saveAsTable(table_name))\n","                print(f\"Appended {new_rows.count()} new rows to {table_name} (hash de-dup).\")\n","\n","print(\"\\nAll files processed successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"37103623-7f11-49e8-bcb2-74737cdc2d70","normalized_state":"finished","queued_time":"2025-09-28T06:20:56.2550384Z","session_start_time":null,"execution_start_time":"2025-09-28T06:20:56.2563554Z","execution_finish_time":"2025-09-28T06:23:12.0826928Z","parent_msg_id":"4fa6de6d-5ea1-4da1-aced-043d68c84cfe"},"text/plain":"StatementMeta(, 37103623-7f11-49e8-bcb2-74737cdc2d70, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Latest folder: 2025-09-28_03-56-31\nPath: abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/f8bc9aa5-a717-4b16-a592-33bcfe0202bb/Files/raw_partition_zone/2025-09-28_03-56-31/\n"]},{"output_type":"stream","name":"stdout","text":["Found 13 CSVs.\n\nProcessing game_plays.csv → game_plays_bronze\n"]},{"output_type":"stream","name":"stdout","text":["Inserted 0 new rows into game_plays_bronze.\n\nProcessing game_shifts.csv → game_shifts_bronze\n"]},{"output_type":"stream","name":"stdout","text":["Inserted 0 new rows into game_shifts_bronze.\n\nProcessing game_plays_players.csv → game_plays_players_bronze\n"]},{"output_type":"stream","name":"stdout","text":["Inserted 0 new rows into game_plays_players_bronze.\n\nProcessing game_skater_stats.csv → game_skater_stats_bronze\nInserted 0 new rows into game_skater_stats_bronze.\n\nProcessing game_penalties.csv → game_penalties_bronze\nInserted 0 new rows into game_penalties_bronze.\n\nProcessing game_goals.csv → game_goals_bronze\nInserted 0 new rows into game_goals_bronze.\n\nProcessing game_goalie_stats.csv → game_goalie_stats_bronze\nInserted 0 new rows into game_goalie_stats_bronze.\n\nProcessing game_teams_stats.csv → game_teams_stats_bronze\nInserted 0 new rows into game_teams_stats_bronze.\n\nProcessing game_officials.csv → game_officials_bronze\nInserted 0 new rows into game_officials_bronze.\n\nProcessing game.csv → game_bronze\nInserted 0 new rows into game_bronze.\n\nProcessing game_scratches.csv → game_scratches_bronze\nInserted 0 new rows into game_scratches_bronze.\n\nProcessing player_info.csv → player_info_bronze\nOverwrote player_info_bronze with 3925 rows.\n\nProcessing team_info.csv → team_info_bronze\nOverwrote team_info_bronze with 33 rows.\n\nAll files processed successfully.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c788082e-5636-4985-bd55-0f23487deda2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb"}],"default_lakehouse":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb","default_lakehouse_name":"Lakehouse_Bronze","default_lakehouse_workspace_id":"dc478dd4-e53e-4f21-add0-2e376dc173fe"}}},"nbformat":4,"nbformat_minor":5}