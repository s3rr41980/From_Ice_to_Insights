# From_Ice_to_Insights

ðŸ§Š From Ice to Insights â€“ NHL Analytics Pipeline
This project simulates a production-level data pipeline using Microsoft Fabric to process, transform, and analyse NHL game data. It implements a Medallion Architecture (Bronze â†’ Silver â†’ Gold) for ingestion, transformation, and visualisation, enabling faster, data-driven insights into player and team performance.
ðŸŽ¯ Goal: Build a robust, scalable, and fault-tolerant data platform to support analytical and scouting use cases for sports performance optimisation.

---

ðŸ“Š ##Data Sources
CSV Data (Azure Blob Storage): Simulated raw game and player datasets from Kaggle representing production ingestion

---

ðŸ›  Tech Stack
Data Platform: Microsoft Fabric, OneLake, Azure Blob
Processing Engine: PySpark, Databricks Runtime
Data Quality & Validation: Great Expectations
Storage Format: Delta Lake
Visualisation: Power BI, SQL Analytics
Security & Compliance: RBAC, RLS/CLS, Data Masking, ZRS (12 9s durability)

---

ðŸ”„ Pipeline Workflow
Ingest CSV files from Azure Blob into Bronze Lakehouse (Landing Zone)
Transform data in Silver Lakehouse (Processed Zone) with cleansing, schema evolution, and deduplication
Load enriched and modelled tables into Gold Lakehouse (Analytics Zone) for consumption
Model fact and dimension tables following a Fact Constellation Schema
Implement SCD Type 2 tracking and Delta Time Travel for historical accuracy
Visualise insights in Power BI for player performance and game analytics

---

ðŸ§© Architecture
Medallion Layers:
Bronze Lakehouse (Landing Zone): Ingest raw CSV data from Azure Blob Storage
Silver Lakehouse (Processed Zone): Cleanse, transform, and standardise data into relational schema
Gold Lakehouse (Analytics Zone): Model facts and dimensions for Power BI visualisation
Environment: Dev â†’ Test â†’ Prod

---

ðŸŒŸ Key Features
Automated Orchestration: Parameterised ingestion and dynamic partitioning
Data Quality: Fault-tolerant, idempotent, and deduplication-enabled workflows
Scalability: Supports incremental and upsert-based loading
Data Lineage: Partitioned landing zone for CDC and SCD Type 2 tracking
Observability: Error alerts, logging, and pipeline recovery
Security: RLS/CLS, masking, and role-based access

---

ðŸŽ“ Skills Demonstrated
Microsoft Fabric Â· Databricks Â· PySpark Â· Azure Blob Â· Delta Lake Â· Power BI Â· SQL Analytics Â· OneLake Â· Great Expectations Â· Data Modelling Â· SCD Type 2 Â· Data Quality Automation
