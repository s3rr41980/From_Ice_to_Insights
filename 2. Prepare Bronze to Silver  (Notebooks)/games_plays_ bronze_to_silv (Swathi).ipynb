{"cells":[{"cell_type":"markdown","source":["## Clean/transform game_plays_bronze -> game_plays_silver\n","\n","##### 1. Import and load table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08c17648-b05d-49d7-906d-185fc5f34d3a"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","from pyspark.sql import functions as F\n","from pyspark.sql import functions as F, Window\n","\n","df = spark.read.format(\"delta\").load(\"Tables/game_plays_bronze\")\n","\n","#display(df.limit(5))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"ee6db201-31b8-4408-aba1-0e4240eaa4ec","normalized_state":"finished","queued_time":"2025-09-29T00:10:12.2056114Z","session_start_time":null,"execution_start_time":"2025-09-29T00:10:12.2066791Z","execution_finish_time":"2025-09-29T00:10:12.9948659Z","parent_msg_id":"799585e9-04b4-49a2-ac3b-e1e062e93fe1"},"text/plain":"StatementMeta(, ee6db201-31b8-4408-aba1-0e4240eaa4ec, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"781102b9-a4a4-431c-b06e-324e09fd5f59"},{"cell_type":"markdown","source":["##### 2. Clean and transform data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386d8c40-1f19-4b98-a46a-493ea91a9dba"},{"cell_type":"code","source":["# 1. Drop unwanted columns\n","df = df.drop(\"st_x\", \"st_y\")\n","\n","# 2. Remove duplicate rows with exact information\n","df = df.dropDuplicates()\n","\n","# 3. Function to convert camelCase / PascalCase to snake_case\n","def to_snake_case(name):\n","    return ''.join(['_' + c.lower() if c.isupper() else c for c in name]).lstrip('_')\n","\n","# Apply to all columns in one line\n","df = df.toDF(*[to_snake_case(c) for c in df.columns])\n","\n","\"\"\"\n","# Explicit rename for special case\n","df = df.withColumnRenamed(\"periodType\", \"period_type\") \\\n","                   .withColumnRenamed(\"periodTime\", \"period_time\") \\\n","                   .withColumnRenamed(\"periodTimeRemaining\", \"period_time_remaining\") \\\n","                   .withColumnRenamed(\"dateTime\", \"date_time\") \\\n","                   .withColumnRenamed(\"secondaryType\", \"secondary_type\")\n","\"\"\"\n","\n","# 4: Cast data types\n","df = (\n","    df  # ✅ Start from df, not df_clean\n","    .withColumn(\"play_id\", F.col(\"play_id\").cast(\"string\"))\n","    .withColumn(\"game_id\", F.col(\"game_id\").cast(\"string\"))\n","    .withColumn(\"team_id_for\", F.col(\"team_id_for\").cast(\"string\"))\n","    .withColumn(\"team_id_against\", F.col(\"team_id_against\").cast(\"string\"))\n","    .withColumn(\"x\", F.col(\"x\").cast(\"int\"))\n","    .withColumn(\"y\", F.col(\"y\").cast(\"int\"))\n","    .withColumn(\"period\", F.col(\"period\").cast(\"int\"))\n","    .withColumn(\"period_time\", F.col(\"period_time\").cast(\"int\"))\n","    .withColumn(\"period_time_remaining\", F.col(\"period_time_remaining\").cast(\"int\"))\n","    .withColumn(\"goals_away\", F.col(\"goals_away\").cast(\"int\"))\n","    .withColumn(\"goals_home\", F.col(\"goals_home\").cast(\"int\"))\n","    .withColumn(\"date_time\", F.to_timestamp(\"date_time\", \"yyyy/mm/dd H:mm\"))\n",")\n","\n","# 5: Replace blank strings with nulls (optional)\n","df = df.select([\n","    F.when(F.col(c) == \"\", None).otherwise(F.col(c)).alias(c) if dtype == \"string\" else F.col(c)\n","    for c, dtype in df.dtypes\n","])\n","\n","# 6: Trim and clean join key to avoid mismatch issues\n","df = df.withColumn(\"play_id\", F.trim(F.col(\"play_id\")))\n","\n","\"\"\"\n","# Show results\n","display(df.limit(10))\n","\n","rows_final = df.count()\n","cols_final = len(df.columns)\n","print(\"\")\n","print(\"Number of rows:\", rows_final)\n","print(\"Number of columns:\", cols_final)\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"ee6db201-31b8-4408-aba1-0e4240eaa4ec","normalized_state":"finished","queued_time":"2025-09-29T00:10:12.2904175Z","session_start_time":null,"execution_start_time":"2025-09-29T00:10:12.9969581Z","execution_finish_time":"2025-09-29T00:10:13.7620161Z","parent_msg_id":"a7172304-a5bb-4496-8c09-21abccca0277"},"text/plain":"StatementMeta(, ee6db201-31b8-4408-aba1-0e4240eaa4ec, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"'\\n# Show results\\ndisplay(df.limit(10))\\n\\nrows_final = df.count()\\ncols_final = len(df.columns)\\nprint(\"\")\\nprint(\"Number of rows:\", rows_final)\\nprint(\"Number of columns:\", cols_final)\\n'"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84e52826-fe1b-4058-bec9-c4fba3688388"},{"cell_type":"markdown","source":["##### 3. Load data to silver table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0412a85d-93ce-485f-ab7d-10fdc7bbc657"},{"cell_type":"code","source":["# Incoming Bronze dataframe\n","source_df = df  \n","key_col = \"play_id\"\n","\n","# Path to Silver Lakehouse Delta table\n","target_path = \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/ce7ef0e9-78af-44db-b5ee-839dcf1c9e98/Tables/games_play_silver\"\n","\n","if DeltaTable.isDeltaTable(spark, target_path):\n","    # Load existing target table\n","    existing_df = spark.read.format(\"delta\").load(target_path).select(key_col).distinct()\n","\n","    # Keep only new keys\n","    new_rows_df = source_df.join(existing_df, on=key_col, how=\"left_anti\")\n","\n","    if new_rows_df.limit(1).count() > 0:\n","        (new_rows_df.write\n","            .format(\"delta\")\n","            .mode(\"append\")\n","            .save(target_path))\n","        print(f\"✅ Appended {new_rows_df.count()} new rows to games_play_silver in Lakehouse_Silver.\")\n","    else:\n","        print(\"No new rows to append. games_play_silver is already up to date.\")\n","else:\n","    # First load → create the Silver table\n","    (source_df.write\n","        .format(\"delta\")\n","        .mode(\"overwrite\")\n","        .save(target_path))\n","    print(f\"✅ Initial load complete: created games_play_silver in Lakehouse_Silver with {source_df.count()} rows.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"ee6db201-31b8-4408-aba1-0e4240eaa4ec","normalized_state":"finished","queued_time":"2025-09-29T00:10:12.3459099Z","session_start_time":null,"execution_start_time":"2025-09-29T00:10:13.7642403Z","execution_finish_time":"2025-09-29T00:10:38.7964648Z","parent_msg_id":"50125624-be9d-49d1-9763-0cb477d36528"},"text/plain":"StatementMeta(, ee6db201-31b8-4408-aba1-0e4240eaa4ec, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["No new rows to append. games_play_silver is already up to date.\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47e2b9f6-6138-4466-a37f-198983306d84"},{"cell_type":"code","source":["\"\"\"\n","# 1. Cast data types\n","df = (\n","    df  # ✅ Start from df, not df_clean\n","    .withColumn(\"play_id\", F.col(\"play_id\").cast(\"string\"))\n","    .withColumn(\"game_id\", F.col(\"game_id\").cast(\"string\"))\n","    .withColumn(\"team_id_for\", F.col(\"team_id_for\").cast(\"string\"))\n","    .withColumn(\"team_id_against\", F.col(\"team_id_against\").cast(\"string\"))\n","    .withColumn(\"x\", F.col(\"x\").cast(\"int\"))\n","    .withColumn(\"y\", F.col(\"y\").cast(\"int\"))\n","    .withColumn(\"st_x\", F.col(\"st_x\").cast(\"int\"))\n","    .withColumn(\"st_y\", F.col(\"st_y\").cast(\"int\"))\n","    .withColumn(\"period\", F.col(\"period\").cast(\"int\"))\n","    .withColumn(\"periodTime\", F.col(\"periodTime\").cast(\"int\"))\n","    .withColumn(\"periodTimeRemaining\", F.col(\"periodTimeRemaining\").cast(\"int\"))\n","    .withColumn(\"goals_away\", F.col(\"goals_away\").cast(\"int\"))\n","    .withColumn(\"goals_home\", F.col(\"goals_home\").cast(\"int\"))\n","    .withColumn(\"dateTime\", F.to_timestamp(\"dateTime\", \"yyyy/mm/dd H:mm\"))\n",")\n","\n","\n","#display(df.limit(5))\n","# 2. Remove duplicate rows\n","df = df.dropDuplicates()\n","# Check for exact duplicate rows\n","#dup_count = df.groupBy(df.columns).count().filter(F.col(\"count\") > 1).count()\n","#print(\"Number of exact duplicated rows:\", dup_count)\n","#print(\"\")\n","\n","# 4. Check schema data type\n","#df.printSchema()\n","#display(df.limit(5))\n","#df_clean.show()\n","# Step 1: Drop unwanted columns\n","df = df.drop(\"st_x\", \"st_y\")\n","\n","# Step 2: Rename columns and convert to snake_case (your existing logic)\n","df = df.withColumnRenamed(\"periodType\", \"period_type\") \\\n","                   .withColumnRenamed(\"periodTime\", \"period_time\") \\\n","                   .withColumnRenamed(\"periodTimeRemaining\", \"period_time_remaining\") \\\n","                   .withColumnRenamed(\"dateTime\", \"date_time\") \\\n","                   .withColumnRenamed(\"secondaryType\", \"secondary_type\")\n","\n","def to_snake_case(name):\n","    return ''.join(['_' + c.lower() if c.isupper() else c for c in name]).lstrip('_')\n","\n","df = df.toDF(*[to_snake_case(c) for c in df.columns])\n","\n","# Step 3: Replace blank strings with nulls (optional)\n","df = df.select([\n","    F.when(F.col(c) == \"\", None).otherwise(F.col(c)).alias(c) if dtype == \"string\" else F.col(c)\n","    for c, dtype in df.dtypes\n","])\n","\n","# Step 4: Trim and clean join key to avoid mismatch issues\n","df = df.withColumn(\"play_id\", F.trim(F.col(\"play_id\")))\n","\"\"\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"ee6db201-31b8-4408-aba1-0e4240eaa4ec","normalized_state":"finished","queued_time":"2025-09-29T00:10:12.413562Z","session_start_time":null,"execution_start_time":"2025-09-29T00:10:38.7990002Z","execution_finish_time":"2025-09-29T00:10:39.0684729Z","parent_msg_id":"b8fff801-6c9d-46df-aeb1-7aa54a4ee1de"},"text/plain":"StatementMeta(, ee6db201-31b8-4408-aba1-0e4240eaa4ec, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"'\\n# 1. Cast data types\\ndf = (\\n    df  # ✅ Start from df, not df_clean\\n    .withColumn(\"play_id\", F.col(\"play_id\").cast(\"string\"))\\n    .withColumn(\"game_id\", F.col(\"game_id\").cast(\"string\"))\\n    .withColumn(\"team_id_for\", F.col(\"team_id_for\").cast(\"string\"))\\n    .withColumn(\"team_id_against\", F.col(\"team_id_against\").cast(\"string\"))\\n    .withColumn(\"x\", F.col(\"x\").cast(\"int\"))\\n    .withColumn(\"y\", F.col(\"y\").cast(\"int\"))\\n    .withColumn(\"st_x\", F.col(\"st_x\").cast(\"int\"))\\n    .withColumn(\"st_y\", F.col(\"st_y\").cast(\"int\"))\\n    .withColumn(\"period\", F.col(\"period\").cast(\"int\"))\\n    .withColumn(\"periodTime\", F.col(\"periodTime\").cast(\"int\"))\\n    .withColumn(\"periodTimeRemaining\", F.col(\"periodTimeRemaining\").cast(\"int\"))\\n    .withColumn(\"goals_away\", F.col(\"goals_away\").cast(\"int\"))\\n    .withColumn(\"goals_home\", F.col(\"goals_home\").cast(\"int\"))\\n    .withColumn(\"dateTime\", F.to_timestamp(\"dateTime\", \"yyyy/mm/dd H:mm\"))\\n)\\n\\n\\n#display(df.limit(5))\\n# 2. Remove duplicate rows\\ndf = df.dropDuplicates()\\n# Check for exact duplicate rows\\n#dup_count = df.groupBy(df.columns).count().filter(F.col(\"count\") > 1).count()\\n#print(\"Number of exact duplicated rows:\", dup_count)\\n#print(\"\")\\n\\n# 4. Check schema data type\\n#df.printSchema()\\n#display(df.limit(5))\\n#df_clean.show()\\n# Step 1: Drop unwanted columns\\ndf = df.drop(\"st_x\", \"st_y\")\\n\\n# Step 2: Rename columns and convert to snake_case (your existing logic)\\ndf = df.withColumnRenamed(\"periodType\", \"period_type\")                    .withColumnRenamed(\"periodTime\", \"period_time\")                    .withColumnRenamed(\"periodTimeRemaining\", \"period_time_remaining\")                    .withColumnRenamed(\"dateTime\", \"date_time\")                    .withColumnRenamed(\"secondaryType\", \"secondary_type\")\\n\\ndef to_snake_case(name):\\n    return \\'\\'.join([\\'_\\' + c.lower() if c.isupper() else c for c in name]).lstrip(\\'_\\')\\n\\ndf = df.toDF(*[to_snake_case(c) for c in df.columns])\\n\\n# Step 3: Replace blank strings with nulls (optional)\\ndf = df.select([\\n    F.when(F.col(c) == \"\", None).otherwise(F.col(c)).alias(c) if dtype == \"string\" else F.col(c)\\n    for c, dtype in df.dtypes\\n])\\n\\n# Step 4: Trim and clean join key to avoid mismatch issues\\ndf = df.withColumn(\"play_id\", F.trim(F.col(\"play_id\")))\\n'"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffcf90ef-3869-4ef2-9457-a7462b9923d4"},{"cell_type":"code","source":["\"\"\"\n","# Welcome to your new notebook\n","# 1. Type here in the cell editor to add code!\n","\n","\n","# 3 . Cast data types\n","df_clean = (\n","    df  # ✅ Start from df, not df_clean\n","    .withColumn(\"play_id\", F.col(\"play_id\").cast(\"string\"))\n","    .withColumn(\"game_id\", F.col(\"game_id\").cast(\"string\"))\n","    .withColumn(\"team_id_for\", F.col(\"team_id_for\").cast(\"string\"))\n","    .withColumn(\"team_id_against\", F.col(\"team_id_against\").cast(\"string\"))\n","    .withColumn(\"x\", F.col(\"x\").cast(\"int\"))\n","    .withColumn(\"y\", F.col(\"y\").cast(\"int\"))\n","    .withColumn(\"st_x\", F.col(\"st_x\").cast(\"int\"))\n","    .withColumn(\"st_y\", F.col(\"st_y\").cast(\"int\"))\n","    .withColumn(\"period\", F.col(\"period\").cast(\"int\"))\n","    .withColumn(\"periodTime\", F.col(\"periodTime\").cast(\"int\"))\n","    .withColumn(\"periodTimeRemaining\", F.col(\"periodTimeRemaining\").cast(\"int\"))\n","    .withColumn(\"goals_away\", F.col(\"goals_away\").cast(\"int\"))\n","    .withColumn(\"goals_home\", F.col(\"goals_home\").cast(\"int\"))\n","    .withColumn(\"dateTime\", F.to_timestamp(\"dateTime\", \"yyyy/mm/dd H:mm\"))\n",")\n","\n","\n","#display(df.limit(5))\n","# 2. Remove duplicate rows\n","df = df.dropDuplicates()\n","# Check for exact duplicate rows\n","dup_count = df.groupBy(df.columns).count().filter(F.col(\"count\") > 1).count()\n","#print(\"Number of exact duplicated rows:\", dup_count)\n","#print(\"\")\n","\n","# 4. Check schema data type\n","df.printSchema()\n","#display(df.limit(5))\n","#df_clean.show()\n","# Step 1: Drop unwanted columns\n","df_clean = df_clean.drop(\"st_x\", \"st_y\")\n","\n","# Step 2: Rename columns and convert to snake_case (your existing logic)\n","df_clean = df_clean.withColumnRenamed(\"periodType\", \"period_type\") \\\n","                   .withColumnRenamed(\"periodTime\", \"period_time\") \\\n","                   .withColumnRenamed(\"periodTimeRemaining\", \"period_time_remaining\") \\\n","                   .withColumnRenamed(\"dateTime\", \"date_time\") \\\n","                   .withColumnRenamed(\"secondaryType\", \"secondary_type\")\n","\n","def to_snake_case(name):\n","    return ''.join(['_' + c.lower() if c.isupper() else c for c in name]).lstrip('_')\n","\n","df_clean = df_clean.toDF(*[to_snake_case(c) for c in df_clean.columns])\n","\n","# Step 3: Replace blank strings with nulls (optional)\n","df_clean = df_clean.select([\n","    F.when(F.col(c) == \"\", None).otherwise(F.col(c)).alias(c) if dtype == \"string\" else F.col(c)\n","    for c, dtype in df_clean.dtypes\n","])\n","\n","# Step 4: Trim and clean join key to avoid mismatch issues\n","df_clean = df_clean.withColumn(\"play_id\", F.trim(F.col(\"play_id\")))\n","# Step 5: Define Silver Delta table path\n","target_path = \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/ce7ef0e9-78af-44db-b5ee-839dcf1c9e98/Tables/games_play_silver\"\n","\n","try:\n","    # Load existing Silver table\n","    delta_tbl = DeltaTable.forPath(spark, target_path)\n","    silver_df = delta_tbl.toDF()\n","\n","    # Align Bronze df schema to Silver df schema by casting columns\n","    for field in silver_df.schema.fields:\n","        if field.name in df_clean.columns:\n","            df_clean = df_clean.withColumn(field.name, F.col(field.name).cast(field.dataType))\n","\n","    # Debug: Check counts and sample keys\n","    print(f\"Bronze rows count: {df_clean.count()}\")\n","    print(f\"Silver rows count: {silver_df.count()}\")\n","    print(\"Bronze play_id sample:\")\n","    df_clean.select(\"play_id\").distinct().show(5, truncate=False)\n","    print(\"Silver play_id sample:\")\n","    silver_df.select(\"play_id\").distinct().show(5, truncate=False)\n","\n","    # Debug: Check for matching keys\n","    common_keys_count = df_clean.join(silver_df, \"play_id\", \"inner\").count()\n","    print(f\"Number of matching play_id between Bronze and Silver: {common_keys_count}\")\n","\n","    # Debug: Show Bronze values for x,y,period_time_remaining before merge\n","    print(\"Sample Bronze data (x, y, period_time_remaining):\")\n","    df_clean.select(\"play_id\", \"x\", \"y\", \"period_time_remaining\").show(10, truncate=False)\n","\n","    # Perform merge/upsert with explicit F.col()\n","    delta_tbl.alias(\"silver\").merge(\n","        df_clean.alias(\"bronze\"),\n","        \"silver.play_id = bronze.play_id\"\n","    ).whenMatchedUpdate(set={\n","        \"x\": F.col(\"bronze.x\"),\n","        \"y\": F.col(\"bronze.y\"),\n","        \"period_time_remaining\": F.col(\"bronze.period_time_remaining\")\n","    }).whenNotMatchedInsertAll().execute()\n","\n","    print(\"✅ Merge (upsert) operation complete: Silver table updated.\")\n","\n","except Exception as e:\n","    print(f\"⚠️ Silver table does not exist or error: {e}\")\n","    # Initial load with schema overwrite to avoid conflicts\n","    df_clean.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(target_path)\n","    print(f\"✅ Initial load complete: Silver table created with {df_clean.count()} rows.\")\n","\n","# Step 6: Show sample data from Silver after merge\n","print(\"Sample data from Silver table after upsert:\")\n","spark.read.format(\"delta\").load(target_path).select(\"play_id\", \"x\", \"y\", \"period_time_remaining\").show(10)\n","df_clean.filter(col(\"play_id\") == \"2017021013_184\") \\\n","        .select(\"play_id\", \"x\", \"y\", \"period_time_remaining\") \\\n","        .show(truncate=False)\n","        # Load the Silver Delta table\n","silver_path = \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/ce7ef0e9-78af-44db-b5ee-839dcf1c9e98/Tables/games_play_silver\"\n","df_silver = spark.read.format(\"delta\").load(silver_path)\n","\n","# Filter for a specific play_id and display key columns\n","df_silver.filter(col(\"play_id\") == \"2017021013_184\") \\\n","         .select(\"play_id\", \"x\", \"y\", \"period_time_remaining\") \\\n","         .show(truncate=False)\n","\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"ee6db201-31b8-4408-aba1-0e4240eaa4ec","normalized_state":"finished","queued_time":"2025-09-29T00:10:12.8068392Z","session_start_time":null,"execution_start_time":"2025-09-29T00:10:39.0705165Z","execution_finish_time":"2025-09-29T00:10:39.3570432Z","parent_msg_id":"9415a17c-5aea-464d-b249-57782c5ca9a4"},"text/plain":"StatementMeta(, ee6db201-31b8-4408-aba1-0e4240eaa4ec, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"'\\n# Welcome to your new notebook\\n# 1. Type here in the cell editor to add code!\\n\\n\\n# 3 . Cast data types\\ndf_clean = (\\n    df  # ✅ Start from df, not df_clean\\n    .withColumn(\"play_id\", F.col(\"play_id\").cast(\"string\"))\\n    .withColumn(\"game_id\", F.col(\"game_id\").cast(\"string\"))\\n    .withColumn(\"team_id_for\", F.col(\"team_id_for\").cast(\"string\"))\\n    .withColumn(\"team_id_against\", F.col(\"team_id_against\").cast(\"string\"))\\n    .withColumn(\"x\", F.col(\"x\").cast(\"int\"))\\n    .withColumn(\"y\", F.col(\"y\").cast(\"int\"))\\n    .withColumn(\"st_x\", F.col(\"st_x\").cast(\"int\"))\\n    .withColumn(\"st_y\", F.col(\"st_y\").cast(\"int\"))\\n    .withColumn(\"period\", F.col(\"period\").cast(\"int\"))\\n    .withColumn(\"periodTime\", F.col(\"periodTime\").cast(\"int\"))\\n    .withColumn(\"periodTimeRemaining\", F.col(\"periodTimeRemaining\").cast(\"int\"))\\n    .withColumn(\"goals_away\", F.col(\"goals_away\").cast(\"int\"))\\n    .withColumn(\"goals_home\", F.col(\"goals_home\").cast(\"int\"))\\n    .withColumn(\"dateTime\", F.to_timestamp(\"dateTime\", \"yyyy/mm/dd H:mm\"))\\n)\\n\\n\\n#display(df.limit(5))\\n# 2. Remove duplicate rows\\ndf = df.dropDuplicates()\\n# Check for exact duplicate rows\\ndup_count = df.groupBy(df.columns).count().filter(F.col(\"count\") > 1).count()\\n#print(\"Number of exact duplicated rows:\", dup_count)\\n#print(\"\")\\n\\n# 4. Check schema data type\\ndf.printSchema()\\n#display(df.limit(5))\\n#df_clean.show()\\n# Step 1: Drop unwanted columns\\ndf_clean = df_clean.drop(\"st_x\", \"st_y\")\\n\\n# Step 2: Rename columns and convert to snake_case (your existing logic)\\ndf_clean = df_clean.withColumnRenamed(\"periodType\", \"period_type\")                    .withColumnRenamed(\"periodTime\", \"period_time\")                    .withColumnRenamed(\"periodTimeRemaining\", \"period_time_remaining\")                    .withColumnRenamed(\"dateTime\", \"date_time\")                    .withColumnRenamed(\"secondaryType\", \"secondary_type\")\\n\\ndef to_snake_case(name):\\n    return \\'\\'.join([\\'_\\' + c.lower() if c.isupper() else c for c in name]).lstrip(\\'_\\')\\n\\ndf_clean = df_clean.toDF(*[to_snake_case(c) for c in df_clean.columns])\\n\\n# Step 3: Replace blank strings with nulls (optional)\\ndf_clean = df_clean.select([\\n    F.when(F.col(c) == \"\", None).otherwise(F.col(c)).alias(c) if dtype == \"string\" else F.col(c)\\n    for c, dtype in df_clean.dtypes\\n])\\n\\n# Step 4: Trim and clean join key to avoid mismatch issues\\ndf_clean = df_clean.withColumn(\"play_id\", F.trim(F.col(\"play_id\")))\\n# Step 5: Define Silver Delta table path\\ntarget_path = \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/ce7ef0e9-78af-44db-b5ee-839dcf1c9e98/Tables/games_play_silver\"\\n\\ntry:\\n    # Load existing Silver table\\n    delta_tbl = DeltaTable.forPath(spark, target_path)\\n    silver_df = delta_tbl.toDF()\\n\\n    # Align Bronze df schema to Silver df schema by casting columns\\n    for field in silver_df.schema.fields:\\n        if field.name in df_clean.columns:\\n            df_clean = df_clean.withColumn(field.name, F.col(field.name).cast(field.dataType))\\n\\n    # Debug: Check counts and sample keys\\n    print(f\"Bronze rows count: {df_clean.count()}\")\\n    print(f\"Silver rows count: {silver_df.count()}\")\\n    print(\"Bronze play_id sample:\")\\n    df_clean.select(\"play_id\").distinct().show(5, truncate=False)\\n    print(\"Silver play_id sample:\")\\n    silver_df.select(\"play_id\").distinct().show(5, truncate=False)\\n\\n    # Debug: Check for matching keys\\n    common_keys_count = df_clean.join(silver_df, \"play_id\", \"inner\").count()\\n    print(f\"Number of matching play_id between Bronze and Silver: {common_keys_count}\")\\n\\n    # Debug: Show Bronze values for x,y,period_time_remaining before merge\\n    print(\"Sample Bronze data (x, y, period_time_remaining):\")\\n    df_clean.select(\"play_id\", \"x\", \"y\", \"period_time_remaining\").show(10, truncate=False)\\n\\n    # Perform merge/upsert with explicit F.col()\\n    delta_tbl.alias(\"silver\").merge(\\n        df_clean.alias(\"bronze\"),\\n        \"silver.play_id = bronze.play_id\"\\n    ).whenMatchedUpdate(set={\\n        \"x\": F.col(\"bronze.x\"),\\n        \"y\": F.col(\"bronze.y\"),\\n        \"period_time_remaining\": F.col(\"bronze.period_time_remaining\")\\n    }).whenNotMatchedInsertAll().execute()\\n\\n    print(\"✅ Merge (upsert) operation complete: Silver table updated.\")\\n\\nexcept Exception as e:\\n    print(f\"⚠️ Silver table does not exist or error: {e}\")\\n    # Initial load with schema overwrite to avoid conflicts\\n    df_clean.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(target_path)\\n    print(f\"✅ Initial load complete: Silver table created with {df_clean.count()} rows.\")\\n\\n# Step 6: Show sample data from Silver after merge\\nprint(\"Sample data from Silver table after upsert:\")\\nspark.read.format(\"delta\").load(target_path).select(\"play_id\", \"x\", \"y\", \"period_time_remaining\").show(10)\\ndf_clean.filter(col(\"play_id\") == \"2017021013_184\")         .select(\"play_id\", \"x\", \"y\", \"period_time_remaining\")         .show(truncate=False)\\n        # Load the Silver Delta table\\nsilver_path = \"abfss://dc478dd4-e53e-4f21-add0-2e376dc173fe@onelake.dfs.fabric.microsoft.com/ce7ef0e9-78af-44db-b5ee-839dcf1c9e98/Tables/games_play_silver\"\\ndf_silver = spark.read.format(\"delta\").load(silver_path)\\n\\n# Filter for a specific play_id and display key columns\\ndf_silver.filter(col(\"play_id\") == \"2017021013_184\")          .select(\"play_id\", \"x\", \"y\", \"period_time_remaining\")          .show(truncate=False)\\n\\n'"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b34a150c-34b3-425a-a4cf-eec8e861b121"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb","known_lakehouses":[{"id":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb"}],"default_lakehouse_name":"Lakehouse_Bronze","default_lakehouse_workspace_id":"dc478dd4-e53e-4f21-add0-2e376dc173fe"}}},"nbformat":4,"nbformat_minor":5}