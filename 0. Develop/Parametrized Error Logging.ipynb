{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from pyspark.sql import functions as F"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"d8f74002-59e8-4816-9ef8-b4e3bbc6d3c5","normalized_state":"finished","queued_time":"2025-09-29T12:29:44.5392273Z","session_start_time":null,"execution_start_time":"2025-09-29T12:29:44.5403095Z","execution_finish_time":"2025-09-29T12:29:44.809701Z","parent_msg_id":"d4b15b84-649a-4344-8036-47eff9a5d351"},"text/plain":"StatementMeta(, d8f74002-59e8-4816-9ef8-b4e3bbc6d3c5, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f182cc30-6bd2-4852-8024-69699ca98289"},{"cell_type":"markdown","source":["## Execute this once to create error log table schema"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09526323-ba69-439d-b557-916e8671e685"},{"cell_type":"code","source":["\"\"\"\n","# One-time execution to create tables\n","\n","# Define schema\n","schema = StructType([\n","    StructField(\"event_time_utc\", TimestampType(), False),\n","    StructField(\"pipeline_name\",  StringType(),   False),\n","    StructField(\"run_id\",         StringType(),   False),\n","    StructField(\"activity_name\",  StringType(),   True),\n","    StructField(\"job_stage\",      StringType(),   True),\n","    StructField(\"status\",         StringType(),   False),\n","    StructField(\"error_code\",     StringType(),   True),\n","    StructField(\"error_message\",  StringType(),   True),\n","    StructField(\"source_object\",  StringType(),   True),\n","])\n","\n","# Empty DF\n","empty_df = spark.createDataFrame([], schema)\n","\n","# Write schema into a managed Delta table in the Lakehouse (default schema)\n","(empty_df\n"," .withColumn(\"event_time_utc\", F.current_timestamp())\n"," .limit(0)  # write no rows\n"," .write\n"," .format(\"delta\")\n"," .mode(\"overwrite\")\n"," .saveAsTable(\"pipeline_failure_log\"))\n","\n","print(\"✅ Table pipeline_failure_log created in your Lakehouse (default schema).\")\n","\"\"\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"46b50cff-5784-4e4b-b921-8fb01a6b2542","normalized_state":"finished","queued_time":"2025-09-28T07:17:16.5199032Z","session_start_time":null,"execution_start_time":"2025-09-28T07:17:16.5210309Z","execution_finish_time":"2025-09-28T07:17:22.7663122Z","parent_msg_id":"715ddf10-dbe3-4845-a2bc-2168ca0f1fa5"},"text/plain":"StatementMeta(, 46b50cff-5784-4e4b-b921-8fb01a6b2542, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Table pipeline_failure_log created in your Lakehouse (default schema).\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6dcb2fd4-764e-44ae-beec-b21139ede93e"},{"cell_type":"markdown","source":["##### Attach this to Notebook in pipeline and ensure correct arguments are passed for this to work"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5414203d-5c3f-4ebb-8988-e5d08a719b5b"},{"cell_type":"code","source":["# ------------------------------\n","# Parametrized Error Logging\n","# ------------------------------\n","\n","# 1) Imports (must exist in THIS notebook)\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import (\n","    StructType, StructField,\n","    StringType, TimestampType\n",")\n","\n","# 2) Safe parameter reader\n","def get_param_or_default(param_name: str, default_value: str = \"N/A\") -> str:\n","    \"\"\"\n","    Return the value of a Fabric notebook parameter if it exists; otherwise a default.\n","    This prevents NameError when running the notebook manually.\n","    \"\"\"\n","    try:\n","        value = eval(param_name)\n","        if value is None or str(value).strip() == \"\":\n","            return default_value\n","        return str(value)\n","    except NameError:\n","        return default_value\n","\n","# 3) Collect inputs\n","pipeline_name_value  = get_param_or_default(\"pipeline_name\")\n","run_id_value         = get_param_or_default(\"run_id\")\n","activity_name_value  = get_param_or_default(\"activity_name\")\n","job_stage_value      = get_param_or_default(\"job_stage\")\n","error_code_value     = get_param_or_default(\"error_code\")\n","error_message_value  = get_param_or_default(\"error_message\")\n","source_object_value  = get_param_or_default(\"source_object\")\n","\n","# 4) Ensure the Lakehouse table exists\n","if not spark.catalog.tableExists(\"pipeline_failure_log\"):\n","    raise RuntimeError(\n","        \"The table 'pipeline_failure_log' was not found in the attached Lakehouse. \"\n","        \"Attach the correct Lakehouse to this notebook activity or create the table first.\"\n","    )\n","\n","# 5) Build a single-row DataFrame using Spark literals (no None anywhere)\n","#    This avoids the 'Argument obj can not be None' error for NOT NULL columns.\n","error_df = (\n","    spark.range(1).select(\n","        F.current_timestamp().alias(\"event_time_utc\"),\n","        F.lit(pipeline_name_value).alias(\"pipeline_name\"),\n","        F.lit(run_id_value).alias(\"run_id\"),\n","        F.lit(activity_name_value).alias(\"activity_name\"),\n","        F.lit(job_stage_value).alias(\"job_stage\"),\n","        F.lit(\"Failed\").alias(\"status\"),\n","        F.lit(error_code_value).alias(\"error_code\"),\n","        F.lit(error_message_value).alias(\"error_message\"),\n","        F.lit(source_object_value).alias(\"source_object\"),\n","    )\n",")\n","\n","# 6) Defensive fill for any accidental nulls in STRING columns\n","#    (event_time_utc is already a timestamp literal above)\n","error_df = error_df.na.fill({\n","    \"pipeline_name\": \"N/A\",\n","    \"run_id\": \"N/A\",\n","    \"status\": \"Failed\",          # keep 'Failed' even if someone passes blank\n","    \"activity_name\": \"N/A\",\n","    \"job_stage\": \"N/A\",\n","    \"error_code\": \"N/A\",\n","    \"error_message\": \"N/A\",\n","    \"source_object\": \"N/A\",\n","})\n","\n","# 7) Append to the Delta table, selecting columns in the exact table order\n","(\n","    error_df\n","    .select(\n","        \"event_time_utc\",\n","        \"pipeline_name\",\n","        \"run_id\",\n","        \"activity_name\",\n","        \"job_stage\",\n","        \"status\",\n","        \"error_code\",\n","        \"error_message\",\n","        \"source_object\",\n","    )\n","    .write\n","    .format(\"delta\")\n","    .mode(\"append\")\n","    .saveAsTable(\"pipeline_failure_log\")\n",")\n","\n","print(f\"✅ Logged failure for pipeline '{pipeline_name_value}' (run '{run_id_value}').\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1e0f863e-911c-47be-82c7-9a6f55d12a17"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb"}],"default_lakehouse":"f8bc9aa5-a717-4b16-a592-33bcfe0202bb","default_lakehouse_name":"Lakehouse_Bronze","default_lakehouse_workspace_id":"dc478dd4-e53e-4f21-add0-2e376dc173fe"}}},"nbformat":4,"nbformat_minor":5}